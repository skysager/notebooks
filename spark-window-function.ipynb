{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sparkmagic.magics extension is already loaded. To reload it, use:\n",
      "  %reload_ext sparkmagic.magics\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fadba5857f849ddb203efe3c824a4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(AttachSessionWidget(children=(HTML(value='<br />'), HBox(childr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>spark-152c7b7e88174f5e8f37b16dea39e7ea</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://lab.skysager.com/livy-session-3-PJe9amsc\">Link</a></td><td><a target=\"_blank\" href=\"https://lab.skysager.com/grafana/explore?left=%5B%22now-6h%22%2C%22now%22%2C%22loki%22%2C%7B%22expr%22%3A%22%7Bspark_app_tag%3D%5C%22livy-session-3-PJe9amsc%5C%22%2Cspark_role%3D%5C%22driver%5C%22%7D%22%7D%2C%7B%22ui%22%3A%5Btrue%2Ctrue%2Ctrue%2C%22exact%22%5D%7D%5D\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession name again proxyUser test owner None.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.0\n"
     ]
    }
   ],
   "source": [
    "println(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `spark use` will be kept as it is\n",
    "- in the `Manage Session`, List all the existing sessions globally. \n",
    "- Any notebook can attach to existing session globally\n",
    "- Once attached, the default execution session will be that attached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Window Function - PySpark \n",
    "\n",
    "**Window** (also, windowing or windowed) functions perform a calculation over a set of rows.\n",
    "It is an important tool to do statistics. Most Databases support Window functions. Spark from version 1.4 start supporting Window functions. \n",
    "\n",
    "Spark Window Functions have the following traits:\n",
    "\n",
    "- perform a calculation over a group of rows, called the **Frame**.\n",
    "- a frame corresponding to the current row\n",
    "- return a new value to for each row by an aggregate/window function\n",
    "- Can use SQL grammar or DataFrame API.\n",
    "\n",
    "Spark supports multiple programming languages as the frontends, Scala, Python, R, and other JVM languages. This article will only cover the usage of Window Functions with PySpark DataFrame API. \n",
    "\n",
    "It is very similar for Scala DataFrame API, except few grammar differences. Please refer to [spark-window-function ](https://medium.com/@RockieYang/spark-window-function-d7ae2f5afcb0) on medium\n",
    "\n",
    "For the usage of Windows function with SQL API, please refer to normal SQL guide.\n",
    "\n",
    "## Import all needed package\n",
    "\n",
    "Few objects/classes will be used in the article. Just import them all here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.expressions.Window\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.functions._\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window \n",
    "import org.apache.spark.sql.types._ \n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset\n",
    "\n",
    "The sample dataset has 4 columns, \n",
    "\n",
    "- `depName`: The department name, 3 distinct value in the dataset.\n",
    "- `empNo`: The identity number for the employee\n",
    "- `name`: The name of the employee\n",
    "- `salary`: The salary of the employee. Most employees have different salaries. While some employees have the same salaries for some demo cases.\n",
    "- `hobby`: The list of hobbies of the employee. This is only used for some of the demos.\n",
    "\n",
    "Here is the sample dataset\n",
    "\n",
    "![sample-dataset](https://knockdata.github.io/images/spark-window-function-sample-dataset.png)\n",
    "\n",
    "The following code can be used to create the sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined class Salary\n",
      "empsalary: org.apache.spark.sql.Dataset[Salary] = [depName: string, empNo: bigint ... 3 more fields]\n",
      "+---------+-----+------+------+---------------+\n",
      "|  depName|empNo|  name|salary|          hobby|\n",
      "+---------+-----+------+------+---------------+\n",
      "|    sales|    1| Alice|  5000|    [game, ski]|\n",
      "|personnel|    2|Olivia|  3900|    [game, ski]|\n",
      "|    sales|    3|  Ella|  4800|   [skate, ski]|\n",
      "|    sales|    4|  Ebba|  4800|    [game, ski]|\n",
      "|personnel|    5| Lilly|  3500|   [climb, ski]|\n",
      "|  develop|    7|Astrid|  4200|    [game, ski]|\n",
      "|  develop|    8|  Saga|  6000|   [kajak, ski]|\n",
      "|  develop|    9| Freja|  4500|  [game, kajak]|\n",
      "|  develop|   10| Wilma|  5200|    [game, ski]|\n",
      "|  develop|   11|  Maja|  5200|[game, farming]|\n",
      "+---------+-----+------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case class Salary(depName: String, \n",
    "  empNo: Long, \n",
    "  name: String, \n",
    "  salary: Long, \n",
    "  hobby: Seq[String]) \n",
    "val empsalary = Seq( \n",
    "  Salary(\"sales\", 1, \"Alice\", 5000, List(\"game\", \"ski\")), \n",
    "  Salary(\"personnel\", 2, \"Olivia\", 3900, List(\"game\", \"ski\")),    \n",
    "  Salary(\"sales\", 3, \"Ella\", 4800, List(\"skate\", \"ski\")), \n",
    "  Salary(\"sales\", 4, \"Ebba\", 4800, List(\"game\", \"ski\")), \n",
    "  Salary(\"personnel\", 5, \"Lilly\", 3500, List(\"climb\", \"ski\")), \n",
    "  Salary(\"develop\", 7, \"Astrid\", 4200, List(\"game\", \"ski\")), \n",
    "  Salary(\"develop\", 8, \"Saga\", 6000, List(\"kajak\", \"ski\")), \n",
    "  Salary(\"develop\", 9, \"Freja\", 4500, List(\"game\", \"kajak\")), \n",
    "  Salary(\"develop\", 10, \"Wilma\", 5200, List(\"game\", \"ski\")), \n",
    "  Salary(\"develop\", 11, \"Maja\", 5200, List(\"game\", \"farming\"))).toDS \n",
    "\n",
    "empsalary.createTempView(\"empsalary\") \n",
    "empsalary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Functions\n",
    "\n",
    "There are hundreds of [general spark functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) in which \n",
    "**Aggregate Functions** and |**Window Functions** categories are related to this case. \n",
    "\n",
    "![spark-function-categories](https://knockdata.github.io/images/spark-window-function-spark-function-categories.png)\n",
    "\n",
    "Functions in other categories are NOT applicable for Spark Window. \n",
    "\n",
    "The following example using the function `array_contains` which is in the category of collection functions. \n",
    "Spark will throw out an exception when running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// val overCategory = Window.partitionBy('depName) \n",
    "// val df = empsalary.withColumn(\n",
    "//   \"average_salary_in_dep\", \n",
    "//   array_contains('hobby, \"game\") over overCategory).withColumn(\n",
    "//   \"total_salary_in_dep\", sum('salary) over overCategory) \n",
    "// df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Frame with `partitionBy`\n",
    "\n",
    "A **Basic Frame** has the following traits.\n",
    "\n",
    "- Created with `Window.partitionBy` on one or more columns\n",
    "- Each row has a corresponding frame\n",
    "- The frame will be the same for every row in the same within the same partition. (**NOTE**: This will NOT be the case with **Ordered** Frame)\n",
    "- Aggregate/Window functions can be applied on each row+frame to generate a single value\n",
    "\n",
    "![basic-window-function](https://knockdata.github.io/images/spark-window-function-basic-window.png)\n",
    "\n",
    "In the example, in the previous graph and the following code, we calculate \n",
    "\n",
    "- using function `avg` to calculate average salary in a department \n",
    "- using function `sum` to calculate total salary in a department\n",
    "\n",
    "Here is the sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@17e3de16\n",
      "df: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 5 more fields]\n",
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "|depName  |empNo|name  |salary|salaries                      |average_salary|total_salary|\n",
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "|develop  |7    |Astrid|4200  |[4200, 6000, 4500, 5200, 5200]|5020          |25100       |\n",
      "|develop  |8    |Saga  |6000  |[4200, 6000, 4500, 5200, 5200]|5020          |25100       |\n",
      "|develop  |9    |Freja |4500  |[4200, 6000, 4500, 5200, 5200]|5020          |25100       |\n",
      "|develop  |10   |Wilma |5200  |[4200, 6000, 4500, 5200, 5200]|5020          |25100       |\n",
      "|develop  |11   |Maja  |5200  |[4200, 6000, 4500, 5200, 5200]|5020          |25100       |\n",
      "|sales    |3    |Ella  |4800  |[4800, 4800, 5000]            |4866          |14600       |\n",
      "|sales    |4    |Ebba  |4800  |[4800, 4800, 5000]            |4866          |14600       |\n",
      "|sales    |1    |Alice |5000  |[4800, 4800, 5000]            |4866          |14600       |\n",
      "|personnel|5    |Lilly |3500  |[3500, 3900]                  |3700          |7400        |\n",
      "|personnel|2    |Olivia|3900  |[3500, 3900]                  |3700          |7400        |\n",
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName) \n",
    "val df = empsalary.withColumn( \n",
    "  \"salaries\", collect_list('salary) over overCategory).withColumn(\n",
    "  \"average_salary\", (avg('salary) over overCategory).cast(\"int\")).withColumn(\n",
    "  \"total_salary\", sum('salary) over overCategory).select(\n",
    "  \"depName\", \"empNo\", \"name\", \"salary\", \n",
    "  \"salaries\", \"average_salary\", \"total_salary\") \n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that column salaries by function `collect_list` has the same values in a window.\n",
    "\n",
    "<!-- readmore -->\n",
    "\n",
    "## Ordered Frame with `partitionBy` and `orderBy`\n",
    "\n",
    "An **Ordered Frame** has the following traits.\n",
    "\n",
    "- Created with `Window.partitionBy` on one or more columns\n",
    "- Followed by `orderBy` on a column\n",
    "- Each row have a corresponding frame\n",
    "- The frame will NOT be the same for every row within the same partition. By default, the frame contains all previous rows and the currentRow.\n",
    "- Aggregate/Window functions can be applied to each row+frame to generate a value\n",
    "\n",
    "Here is the sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\n",
      "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@6c2c8df5\n",
      "df: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 5 more fields]\n",
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "|depName  |empNo|name  |salary|salaries                      |average_salary|total_salary|\n",
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "|develop  |8    |Saga  |6000  |[6000]                        |6000          |6000        |\n",
      "|develop  |10   |Wilma |5200  |[6000, 5200, 5200]            |5466          |16400       |\n",
      "|develop  |11   |Maja  |5200  |[6000, 5200, 5200]            |5466          |16400       |\n",
      "|develop  |9    |Freja |4500  |[6000, 5200, 5200, 4500]      |5225          |20900       |\n",
      "|develop  |7    |Astrid|4200  |[6000, 5200, 5200, 4500, 4200]|5020          |25100       |\n",
      "|sales    |1    |Alice |5000  |[5000]                        |5000          |5000        |\n",
      "|sales    |3    |Ella  |4800  |[5000, 4800, 4800]            |4866          |14600       |\n",
      "|sales    |4    |Ebba  |4800  |[5000, 4800, 4800]            |4866          |14600       |\n",
      "|personnel|2    |Olivia|3900  |[3900]                        |3900          |3900        |\n",
      "|personnel|5    |Lilly |3500  |[3900, 3500]                  |3700          |7400        |\n",
      "+---------+-----+------+------+------------------------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).orderBy(\n",
    "  'salary desc) \n",
    "val df = empsalary.withColumn( \n",
    "  \"salaries\", \n",
    "  collect_list('salary) over overCategory).withColumn( \n",
    "  \"average_salary\", \n",
    "  (avg('salary) over overCategory).cast(\"int\")).withColumn( \n",
    "  \"total_salary\", sum('salary) over overCategory).select(\n",
    "  \"depName\", \"empNo\", \"name\", \"salary\", \n",
    "  \"salaries\", \"average_salary\", \"total_salary\") \n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output we can see that column salaries by function `collect_list` does NOT have the same values in a window. \n",
    "The values are only from `unboundedPreceding` until `currentRow`. \n",
    "\n",
    "The `average_salary` and `total_salary` are not over the whole department, but average and total for the salary higher or equal than `currentRow`'s salary.\n",
    "\n",
    "## Rank functions in a group\n",
    "\n",
    "Here is a table of all the rank functions supported in Spark.\n",
    "\n",
    "![rank-functions](https://knockdata.github.io/images/spark-window-function-rank-functions.png)\n",
    "\n",
    "Here is the sample code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\n",
      "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@46ab5516\n",
      "df: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 7 more fields]\n",
      "+---------+-----+------+------+----+----------+----------+-----+------------+\n",
      "|depName  |empNo|name  |salary|rank|dense_rank|row_number|ntile|percent_rank|\n",
      "+---------+-----+------+------+----+----------+----------+-----+------------+\n",
      "|develop  |8    |Saga  |6000  |1   |1         |1         |1    |0.0         |\n",
      "|develop  |10   |Wilma |5200  |2   |2         |2         |1    |0.25        |\n",
      "|develop  |11   |Maja  |5200  |2   |2         |3         |2    |0.25        |\n",
      "|develop  |9    |Freja |4500  |4   |3         |4         |2    |0.75        |\n",
      "|develop  |7    |Astrid|4200  |5   |4         |5         |3    |1.0         |\n",
      "|sales    |1    |Alice |5000  |1   |1         |1         |1    |0.0         |\n",
      "|sales    |3    |Ella  |4800  |2   |2         |2         |2    |0.5         |\n",
      "|sales    |4    |Ebba  |4800  |2   |2         |3         |3    |0.5         |\n",
      "|personnel|2    |Olivia|3900  |1   |1         |1         |1    |0.0         |\n",
      "|personnel|5    |Lilly |3500  |2   |2         |2         |2    |1.0         |\n",
      "+---------+-----+------+------+----+----------+----------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).orderBy(\n",
    "  'salary desc) \n",
    "val df = empsalary.withColumn( \n",
    "  \"salaries\", collect_list('salary) over overCategory).withColumn(\n",
    "  \"rank\", rank() over overCategory).withColumn(\n",
    "  \"dense_rank\", dense_rank() over overCategory).withColumn(\n",
    "  \"row_number\", row_number() over overCategory).withColumn(\n",
    "  \"ntile\", ntile(3) over overCategory).withColumn(\n",
    "  \"percent_rank\", percent_rank() over overCategory).select(\n",
    "  \"depName\", \"empNo\", \"name\", \"salary\", \n",
    "  \"rank\", \"dense_rank\", \"row_number\", \"ntile\", \"percent_rank\") \n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the rank function, we can easily filter to get the rows we want. \n",
    "\n",
    "The following example keeps the top 2 employees salary wise, others have to go. \n",
    "On the sample dataset, Wilma and Maja have the same salary. \n",
    "Maja has to go according to order, unfortunately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\n",
      "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3f0e98ab\n",
      "df: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 2 more fields]\n",
      "+---------+-----+------+------+\n",
      "|depName  |empNo|name  |salary|\n",
      "+---------+-----+------+------+\n",
      "|develop  |8    |Saga  |6000  |\n",
      "|develop  |10   |Wilma |5200  |\n",
      "|sales    |1    |Alice |5000  |\n",
      "|sales    |3    |Ella  |4800  |\n",
      "|personnel|2    |Olivia|3900  |\n",
      "|personnel|5    |Lilly |3500  |\n",
      "+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).orderBy(\n",
    "  'salary desc) \n",
    "val df = empsalary.withColumn(\n",
    "  \"row_number\", row_number() over overCategory).filter( \n",
    "  'row_number <= 2).select(\n",
    "  \"depName\", \"empNo\", \"name\", \"salary\") \n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lag & lead in a group\n",
    "\n",
    "`lag` and `lead` can be used, when we want to get a relative result between rows.\n",
    "The real values we get are depending on the order. \n",
    "\n",
    "`lag` means getting the value from the previous row; `lead` means getting the value from the next row.\n",
    " \n",
    "The following example adding rows with lead and lag salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@6aabf8f2\n",
      "df: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 4 more fields]\n",
      "+---------+-----+------+------+----+----+\n",
      "|depName  |empNo|name  |salary|lead|lag |\n",
      "+---------+-----+------+------+----+----+\n",
      "|develop  |8    |Saga  |6000  |5200|null|\n",
      "|develop  |10   |Wilma |5200  |5200|6000|\n",
      "|develop  |11   |Maja  |5200  |4500|5200|\n",
      "|develop  |9    |Freja |4500  |4200|5200|\n",
      "|develop  |7    |Astrid|4200  |null|4500|\n",
      "|sales    |1    |Alice |5000  |4800|null|\n",
      "|sales    |3    |Ella  |4800  |4800|5000|\n",
      "|sales    |4    |Ebba  |4800  |null|4800|\n",
      "|personnel|2    |Olivia|3900  |3500|null|\n",
      "|personnel|5    |Lilly |3500  |null|3900|\n",
      "+---------+-----+------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depname).orderBy(\n",
    "  'salary.desc) \n",
    "val df = empsalary.withColumn( \n",
    "    \"lead\", lead('salary, 1).over(overCategory)).withColumn( \n",
    "    \"lag\", lag('salary, 1).over(overCategory)).select( \n",
    "    \"depName\", \"empNo\", \"name\", \"salary\", \"lead\", \"lag\") \n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the output, the first row in a window with `lag` will have value **null**, and the last row in a window with `lead` will have value **null**\n",
    "\n",
    "We can calculate the difference with `lead` and `lag` compare the `currentRow`. While since either the first/last value will be **null**, so one of difference value will be **null**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 6 more fields]\n",
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "|  depName|empNo|  name|salary|lead| lag|higher_than_next|lower_than_previous|\n",
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "|  develop|    8|  Saga|  6000|5200|null|             800|               null|\n",
      "|  develop|   10| Wilma|  5200|5200|6000|               0|                800|\n",
      "|  develop|   11|  Maja|  5200|4500|5200|             700|                  0|\n",
      "|  develop|    9| Freja|  4500|4200|5200|             300|                700|\n",
      "|  develop|    7|Astrid|  4200|null|4500|            null|                300|\n",
      "|    sales|    1| Alice|  5000|4800|null|             200|               null|\n",
      "|    sales|    3|  Ella|  4800|4800|5000|               0|                200|\n",
      "|    sales|    4|  Ebba|  4800|null|4800|            null|                  0|\n",
      "|personnel|    2|Olivia|  3900|3500|null|             400|               null|\n",
      "|personnel|    5| Lilly|  3500|null|3900|            null|                400|\n",
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val diff = df.withColumn( \n",
    "    \"higher_than_next\", 'salary - 'lead).withColumn( \n",
    "    \"lower_than_previous\", 'lag - 'salary) \n",
    "diff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace the value **null** after getting the difference. Or like this example, using `when` to calculate the difference, fill in a literal value, e.g 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 6 more fields]\n",
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "|  depName|empNo|  name|salary|lead| lag|higher_than_next|lower_than_previous|\n",
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "|  develop|    8|  Saga|  6000|5200|null|             800|                  0|\n",
      "|  develop|   10| Wilma|  5200|5200|6000|               0|                800|\n",
      "|  develop|   11|  Maja|  5200|4500|5200|             700|                  0|\n",
      "|  develop|    9| Freja|  4500|4200|5200|             300|                700|\n",
      "|  develop|    7|Astrid|  4200|null|4500|               0|                300|\n",
      "|    sales|    1| Alice|  5000|4800|null|             200|                  0|\n",
      "|    sales|    3|  Ella|  4800|4800|5000|               0|                200|\n",
      "|    sales|    4|  Ebba|  4800|null|4800|               0|                  0|\n",
      "|personnel|    2|Olivia|  3900|3500|null|             400|                  0|\n",
      "|personnel|    5| Lilly|  3500|null|3900|               0|                400|\n",
      "+---------+-----+------+------+----+----+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val diff = df.withColumn( \n",
    "  \"higher_than_next\", \n",
    "  when('lead.isNull, 0).otherwise('salary - 'lead)).withColumn( \n",
    "  \"lower_than_previous\", \n",
    "  when('lag.isNull, 0).otherwise('lag - 'salary)) \n",
    "diff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, after calculating the difference, find some outliers which have a huge salary gap. \n",
    "The following example takes employees whose salary is double to the next employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+------+----+---+----------------+-------------------+\n",
      "|depName|empNo|name|salary|lead|lag|higher_than_next|lower_than_previous|\n",
      "+-------+-----+----+------+----+---+----------------+-------------------+\n",
      "+-------+-----+----+------+----+---+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff.filter(col(\"higher_than_next\") > (lit(2) * col(\"salary\"))).show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Total \n",
    "\n",
    "**Running Total** means adding everything up until the `currentRow`\n",
    "\n",
    "Example code to calculate running total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\n",
      "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@1e66e308\n",
      "running_total: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 4 more fields]\n",
      "+---------+-----+------+------+----+-----+\n",
      "|depName  |empNo|name  |salary|rank|costs|\n",
      "+---------+-----+------+------+----+-----+\n",
      "|develop  |8    |Saga  |6000  |1   |6000 |\n",
      "|develop  |10   |Wilma |5200  |2   |16400|\n",
      "|develop  |11   |Maja  |5200  |2   |16400|\n",
      "|develop  |9    |Freja |4500  |4   |20900|\n",
      "|develop  |7    |Astrid|4200  |5   |25100|\n",
      "|sales    |1    |Alice |5000  |1   |5000 |\n",
      "|sales    |3    |Ella  |4800  |2   |14600|\n",
      "|sales    |4    |Ebba  |4800  |2   |14600|\n",
      "|personnel|2    |Olivia|3900  |1   |3900 |\n",
      "|personnel|5    |Lilly |3500  |2   |7400 |\n",
      "+---------+-----+------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depname).orderBy(\n",
    "  'salary desc) \n",
    "val running_total = empsalary.withColumn( \n",
    "  \"rank\", rank().over(overCategory)).withColumn( \n",
    "  \"costs\", sum('salary).over(overCategory)).select( \n",
    "  \"depName\", \"empNo\", \"name\", \"salary\", \"rank\", \"costs\") \n",
    "running_total.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the example behavior we want, we might get row_number first, then calculate the running total.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\n",
      "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3960e7cb\n",
      "overRowCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5d8d9660\n",
      "running_total: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 4 more fields]\n",
      "+---------+-----+------+------+----------+-----+\n",
      "|depName  |empNo|name  |salary|row_number|costs|\n",
      "+---------+-----+------+------+----------+-----+\n",
      "|develop  |8    |Saga  |6000  |1         |6000 |\n",
      "|develop  |10   |Wilma |5200  |2         |11200|\n",
      "|develop  |11   |Maja  |5200  |3         |16400|\n",
      "|develop  |9    |Freja |4500  |4         |20900|\n",
      "|develop  |7    |Astrid|4200  |5         |25100|\n",
      "|sales    |1    |Alice |5000  |1         |5000 |\n",
      "|sales    |3    |Ella  |4800  |2         |9800 |\n",
      "|sales    |4    |Ebba  |4800  |3         |14600|\n",
      "|personnel|2    |Olivia|3900  |1         |3900 |\n",
      "|personnel|5    |Lilly |3500  |2         |7400 |\n",
      "+---------+-----+------+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depname).orderBy(\n",
    "  'salary desc) \n",
    "val overRowCategory = Window.partitionBy('depname).orderBy(\n",
    "  'row_number) \n",
    "val running_total = empsalary.withColumn( \n",
    "    \"row_number\", row_number() over overCategory).withColumn( \n",
    "    \"costs\", sum('salary) over overRowCategory).select( \n",
    "    \"depName\", \"empNo\", \"name\", \"salary\", \"row_number\", \"costs\") \n",
    "running_total.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range Frame\n",
    "\n",
    "We can use range functions to change frame boundary. \n",
    "\n",
    "- Create with `Window.partitionBy` on one or more columns\n",
    "- It usually has `orderBy` so that the data in the frame is ordered.\n",
    "- Then followed by `rangeBetween` or `rowsBetween`\n",
    "- Each row will have a corresponding frame\n",
    "- Frame boundary can be controlled by `rangeBetween` or `rowsBetween`\n",
    "- Aggregate/Window functions can be applied on each row+frame to generate a single value\n",
    "\n",
    "There are two range window functions, here are the functions definitions\n",
    "\n",
    "    rowsBetween( start, end)\n",
    "    \n",
    "    rangeBetween(start, end)\n",
    "\n",
    "both functions accept two parameters, [start, end] all inclusive. The parameters value can be `Window.unboundedPreceding`, `Window.unboundedFollowing`, and `Window.currentRow`. Or a value relative to `Window.currentRow`, either negtive or positive.\n",
    "\n",
    "`rowsBetween` get the frame boundary based on the row index in the window compared to `currentRow`. here are a few examples and it's meaning. \n",
    "\n",
    "![range-frame](https://knockdata.github.io/images/spark-window-function-range-frame.png)\n",
    "\n",
    "`rangeBetween` get the frame boundary based on row **value** in the window compared to `currentRow`. The difference compares to `rowsBetween` is that it compare with **value** of the current row. \n",
    "\n",
    "Here is the value definition of the constant values used in range functions.\n",
    "\n",
    "> Window.currentRow = 0\n",
    "> \n",
    "> Window.unboundedPreceding = Long.MinValue\n",
    ">\n",
    "> Window.unboundedFollowing = Long.MaxValue\n",
    "\n",
    "Here is an example use directly after `Window.partitionBy`, without an `orderBy`. We can see from the output that the data in the window is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@43a9f09f\n",
      "df: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 5 more fields]\n",
      "+---------+-----+------+------+------------+------------+\n",
      "|depName  |empNo|name  |salary|salaries    |total_salary|\n",
      "+---------+-----+------+------+------------+------------+\n",
      "|develop  |9    |Freja |4500  |[4500, 5200]|9700        |\n",
      "|develop  |10   |Wilma |5200  |[5200, 5200]|10400       |\n",
      "|develop  |11   |Maja  |5200  |[5200, 4200]|9400        |\n",
      "|develop  |7    |Astrid|4200  |[4200, 6000]|10200       |\n",
      "|develop  |8    |Saga  |6000  |[6000]      |6000        |\n",
      "|sales    |1    |Alice |5000  |[5000, 4800]|9800        |\n",
      "|sales    |3    |Ella  |4800  |[4800, 4800]|9600        |\n",
      "|sales    |4    |Ebba  |4800  |[4800]      |4800        |\n",
      "|personnel|2    |Olivia|3900  |[3900, 3500]|7400        |\n",
      "|personnel|5    |Lilly |3500  |[3500]      |3500        |\n",
      "+---------+-----+------+------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).rowsBetween(\n",
    "  Window.currentRow, 1) \n",
    "val df = empsalary.withColumn( \n",
    "  \"salaries\", collect_list('salary) over overCategory).withColumn( \n",
    "  \"total_salary\", sum('salary) over overCategory) \n",
    "df.select(\"depName\", \"empNo\", \"name\", \"salary\", \n",
    "  \"salaries\", \"total_salary\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example use after `Window.partitionBy` and `orderBy`. The data in the window is ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one feature warning; for details, enable `:setting -feature' or `:replay -feature'\n",
      "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@1185ee0d\n",
      "df: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 5 more fields]\n",
      "+---------+-----+------+------+------------+------------+\n",
      "|depName  |empNo|name  |salary|salaries    |total_salary|\n",
      "+---------+-----+------+------+------------+------------+\n",
      "|develop  |8    |Saga  |6000  |[6000, 5200]|11200       |\n",
      "|develop  |10   |Wilma |5200  |[5200, 5200]|10400       |\n",
      "|develop  |11   |Maja  |5200  |[5200, 4500]|9700        |\n",
      "|develop  |9    |Freja |4500  |[4500, 4200]|8700        |\n",
      "|develop  |7    |Astrid|4200  |[4200]      |4200        |\n",
      "|sales    |1    |Alice |5000  |[5000, 4800]|9800        |\n",
      "|sales    |3    |Ella  |4800  |[4800, 4800]|9600        |\n",
      "|sales    |4    |Ebba  |4800  |[4800]      |4800        |\n",
      "|personnel|2    |Olivia|3900  |[3900, 3500]|7400        |\n",
      "|personnel|5    |Lilly |3500  |[3500]      |3500        |\n",
      "+---------+-----+------+------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).orderBy(\n",
    "  'salary desc).rowsBetween( Window.currentRow, 1) \n",
    "val df = empsalary.withColumn( \n",
    "    \"salaries\", collect_list('salary) over overCategory).withColumn( \n",
    "    \"total_salary\", sum('salary) over overCategory) \n",
    "df.select(\"depName\", \"empNo\", \"name\", \"salary\", \n",
    "  \"salaries\", \"total_salary\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median\n",
    "\n",
    "`mean`(`avg`) and `median` are commonly used in statistics. In certain cases `median` are more robust comparing to mean, since it will filter out outlier values.\n",
    "\n",
    "![mean-median](https://knockdata.github.io/images/spark-window-function-mean-median.png)\n",
    "\n",
    "We can either using Window function directly or first calculate the median value, then join back with the original data frame.\n",
    "\n",
    "### Use Window to calculate median\n",
    "\n",
    "\n",
    "[element_at](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.element_at) function in pyspark does not accept column as the second parameter. We can define a UDF(User Defined Function) to get the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "// @udf(\"long\")\n",
    "// def median_udf(s):\n",
    "//   index = int(len(s) / 2)\n",
    "//   return s[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: There will be **performance impact** using UDF function.\n",
    "\n",
    "We can use window function to calculate the median value. Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overCategory: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@60962b43\n",
      "df: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 5 more fields]\n",
      "+---------+-----+------+------+------------------------------+-------------+\n",
      "|depName  |empNo|name  |salary|salaries                      |median_salary|\n",
      "+---------+-----+------+------+------------------------------+-------------+\n",
      "|develop  |7    |Astrid|4200  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |9    |Freja |4500  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |10   |Wilma |5200  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |11   |Maja  |5200  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |8    |Saga  |6000  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|sales    |3    |Ella  |4800  |[4800, 4800, 5000]            |4800         |\n",
      "|sales    |4    |Ebba  |4800  |[4800, 4800, 5000]            |4800         |\n",
      "|sales    |1    |Alice |5000  |[4800, 4800, 5000]            |4800         |\n",
      "|personnel|5    |Lilly |3500  |[3500, 3900]                  |3900         |\n",
      "|personnel|2    |Olivia|3900  |[3500, 3900]                  |3900         |\n",
      "+---------+-----+------+------+------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val overCategory = Window.partitionBy('depName).orderBy(\n",
    "  'salary).rowsBetween( \n",
    "  Window.unboundedPreceding, Window.unboundedFollowing) \n",
    "val df = empsalary.withColumn( \n",
    "  \"salaries\", collect_list('salary) over overCategory).withColumn(\n",
    "  \"median_salary\", \n",
    "  element_at('salaries, (size('salaries)/2 + 1).cast(\"int\"))) \n",
    "df.select(\"depName\", \"empNo\", \"name\", \"salary\", \"salaries\", \"median_salary\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use groupBy then join back to calculate the median value\n",
    "\n",
    "We can calculate the median value first, then join back with the original DataFrame. Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfMedian: org.apache.spark.sql.DataFrame = [depName: string, salaries: array<bigint> ... 1 more field]\n",
      "df: org.apache.spark.sql.DataFrame = [depName: string, empNo: bigint ... 4 more fields]\n",
      "+---------+-----+------+------+------------------------------+-------------+\n",
      "|depName  |empNo|name  |salary|salaries                      |median_salary|\n",
      "+---------+-----+------+------+------------------------------+-------------+\n",
      "|sales    |1    |Alice |5000  |[4800, 4800, 5000]            |4800         |\n",
      "|personnel|2    |Olivia|3900  |[3500, 3900]                  |3900         |\n",
      "|sales    |3    |Ella  |4800  |[4800, 4800, 5000]            |4800         |\n",
      "|sales    |4    |Ebba  |4800  |[4800, 4800, 5000]            |4800         |\n",
      "|personnel|5    |Lilly |3500  |[3500, 3900]                  |3900         |\n",
      "|develop  |7    |Astrid|4200  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |8    |Saga  |6000  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |9    |Freja |4500  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |10   |Wilma |5200  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "|develop  |11   |Maja  |5200  |[4200, 4500, 5200, 5200, 6000]|5200         |\n",
      "+---------+-----+------+------+------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val dfMedian = empsalary.groupBy(\"depName\").agg( \n",
    "  sort_array(collect_list('salary)).as(\"salaries\")).select( \n",
    "  'depName, 'salaries, \n",
    "  element_at('salaries, (size('salaries)/2 + 1).cast(\"int\")).as(\"median_salary\")) \n",
    "val df = empsalary.join(broadcast(dfMedian), \"depName\").select(\n",
    "  \"depName\", \"empNo\", \"name\", \"salary\", \"salaries\", \"median_salary\") \n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Spark API WindowSpec](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.expressions.WindowSpec)\n",
    "- [Spark API functions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions)\n",
    "- [mastering-spark-sql](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html)\n",
    "- [Introducing Window Functions in Spark SQL](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
